---

# ğŸ§  How Our Model Works (and Why We Ended Up Using Logistic Regression)

So for this project, we tried a few different models â€” Decision Tree, Random Forest, Naive Bayes, and Logistic Regression â€” to see which one could best identify whether each word in a sentence is Filipino, English, or Other.

After a lot of testing, we decided to go with **Logistic Regression**, because it gave the most balanced and consistent results across validation and test data.

---

## âš™ï¸ How Logistic Regression Actually Works

Logistic Regression isnâ€™t actually â€œregressionâ€ like predicting numbers â€” itâ€™s a **classifier**.
In our case, itâ€™s predicting the **probability** that a word is `FIL`, `ENG`, or `OTH`.

It works using the **features** we made in `feature_utils.py` â€” things like:

* whether the word starts with `mag` or `nag`
* if it ends with `ing` or `ed`
* if it contains `ng`, `tion`, or `mga`
* if itâ€™s all caps, a number, or has punctuation

Each of those acts as a **clue** that hints at which language the word belongs to.

During training, the model assigns a **weight** to each clue.
For example:

* `starts_mag` might strongly point to Filipino
* `ends_ing` might point to English
* `has_ng` could also suggest Filipino

So when you input a word like `"magtraining"`, the model multiplies each feature by its weight, adds everything up, and converts it into probabilities â€” for example:

> FIL: 0.85, ENG: 0.10, OTH: 0.05

Then it picks the label with the highest probability.
Thatâ€™s literally how our final predictions are made.

---

## ğŸ” What Happens During Training

When training starts, the model doesnâ€™t know anything â€” all weights start randomly.
It predicts the wrong labels at first, checks how wrong it was, and slightly adjusts the weights.
This repeats over and over (we set `max_iter=1000`) until the weights stabilize and the predictions become accurate.

By the end, it has learned how strongly each feature affects each label â€”
for example, how much â€œends_ingâ€ contributes to being English versus Filipino.

---

## ğŸ’¡ Why Logistic Regression Performed Better Than Decision Trees and Random Forests

We also tried Decision Trees and Random Forests, but Logistic Regression performed better for a few reasons.

| Model                   | How It Works                                                    | Why It Didnâ€™t Fit Our Data                                                         |
| ----------------------- | --------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Decision Tree**       | Makes strict â€œif-elseâ€ splits (e.g., if `ends_ing=1` â†’ English) | Overfits easily â€” memorizes the training data instead of learning general patterns |
| **Random Forest**       | Combines many trees to reduce overfitting                       | Still struggles when features overlap or depend on each other                      |
| **Logistic Regression** | Uses all features together and finds weighted probabilities     | Handles overlapping or dependent features smoothly                                 |

Filipino-English code-switching isnâ€™t rule-based. Some Filipino words look English, and some English words borrow Filipino spelling. Because Decision Trees make hard boundaries, they canâ€™t deal with that overlap. Logistic Regression, on the other hand, draws a **soft decision boundary** â€” it balances probabilities instead of committing to a single hard rule.

---

## ğŸ¤– Why We Didnâ€™t Use Naive Bayes

Naive Bayes assumes all features are **independent**, meaning it thinks each feature affects the label separately.
Thatâ€™s fine for something like spam detection (â€œhas freeâ€, â€œhas winâ€, â€œhas moneyâ€), but it doesnâ€™t make sense for language classification.

In our dataset, features are clearly **dependent** â€” they interact with each other.
For example:

* `starts_mag=1` (Filipino clue) and `ends_ing=1` (English clue) appearing together might actually mean **code-switched Filipino-English**.
* `has_ng` and `has_mga` often occur in the same word â€” they reinforce each other, not act independently.

Naive Bayes doesnâ€™t account for that â€” it just multiplies independent probabilities.
So it oversimplifies our linguistic features and ends up guessing wrong more often.

Logistic Regression fixes that by **combining** all features together, giving each one a weight thatâ€™s learned in context with all the others.

---

## âš–ï¸ So, What Does â€œDependent Featuresâ€ Mean in Our Case?

It basically means some features only make sense **when seen together**.

Example:

* â€œends_ingâ€ alone might mean English â€”
  but if itâ€™s also â€œstarts_magâ€, itâ€™s probably a Filipino-English mixed word.
* â€œis_capitalizedâ€ isnâ€™t useful by itself â€”
  but combined with â€œhas_punctâ€ or â€œhas_digitâ€, it might hint at something like â€œOTHâ€ (symbols, names, or expressions).

Because our linguistic clues rely on one another, theyâ€™re **dependent**.
Thatâ€™s why a model that can handle relationships between features â€” like Logistic Regression â€” performs better than one that assumes independence (like Naive Bayes).

---

## ğŸ In Simple Terms

* Logistic Regression looks at **all clues together** and learns how strong each one is.
* Decision Trees and Random Forests rely on **hard yes/no rules**, which overfit easily.
* Naive Bayes assumes **clues donâ€™t interact**, which doesnâ€™t match real language patterns.
* Our features (prefixes, suffixes, capitalization, etc.) are **dependent**, so Logistic Regression makes more sense.

---

âœ… **Final Takeaway**
Logistic Regression gave us the best results because it generalizes patterns in code-switched text instead of memorizing or simplifying them. It learns how combinations of Filipino and English language features work together â€” which is exactly what we needed for a bilingual word classifier.

---

Would you like me to add a **small closing paragraph** that summarizes this whole section in one short â€œWhat we learned from testing different modelsâ€ paragraph (like a reflection-style ending)? Itâ€™d be perfect for the last part of your README or presentation.

